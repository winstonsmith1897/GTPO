
# GTPO: Trajectory-Based Policy Optimization in Large Language Models

This repository contains the official implementation of **GTPO (Group-relative Trajectory-based Policy Optimization)**, a novel method for stable and effective policy optimization in Large Language Models (LLMs).  
GTPO addresses key limitations of Group-relative Policy Optimization (GRPO), namely:

1. **Token-level gradient conflicts** â€“ where tokens shared across positively and negatively rewarded completions are inconsistently updated, often penalizing essential formatting tokens.
2. **Policy collapse** â€“ where negatively rewarded completions destabilize training, flattening the output distribution and degrading performance.

GTPO introduces **conflict-aware gradient corrections** and **entropy-based regularization** to mitigate these issues, ensuring more stable training without the need for KL-divergence regularization or a reference model.

ğŸ“„ Paper: [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)  
*(Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino)*

---

## ğŸš€ Key Contributions

- **Conflict-Aware Gradient Correction**  
  Identifies and protects conflict tokens (appearing in both positively and negatively rewarded completions), preventing harmful penalization while reinforcing beneficial updates.

- **Entropy-Based Policy Regularization**  
  Filters unstable, high-entropy completions and introduces entropy-based penalties to prevent policy collapse and stabilize training.

- **No Reference Model Required**  
  Unlike GRPO, GTPO removes the dependency on KL divergence and external reference policies, reducing computational overhead.

- **Empirical Validation**  
  Extensive experiments on **GSM8K**, **MATH**, and **AIME 2024** benchmarks using **LLaMA-8B** and **Qwen 2.5-3B** demonstrate:
  - More stable training dynamics  
  - Improved reasoning accuracy  
  - Stronger out-of-distribution generalization  

---

## ğŸ“Š Results

- GTPO consistently outperforms GRPO and SFT in **accuracy**, **formatting consistency**, and **pass@k/maj@k metrics**.  
- Demonstrates robustness against policy collapse, maintaining performance across long training runs.  
- Out-of-distribution evaluation on **AIME 2024** shows significant improvements in reasoning generalization.  

---

## âš™ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/<your-org-or-username>/GTPO.git
cd GTPO
````

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ‹ï¸ Training

Example training script with GTPO:

```bash
python train.py \
  --model llama-8b \
  --dataset gsm8k \
  --optimizer adam \
  --lr 1e-6 \
  --epochs 3 \
  --batch_size 2 \
  --generation_size 12 \
  --use_gtpo
```

Options:

* `--use_grpo` : Train with GRPO baseline
* `--use_sft`  : Train with SFT baseline
* `--generation_size` : Number of completions per group (e.g., 8 or 12)

---

## ğŸ“‚ Repository Structure

```
GTPO/
â”‚â”€â”€ gtpo/               # Core GTPO implementation
â”‚â”€â”€ scripts/            # Training and evaluation scripts
â”‚â”€â”€ configs/            # Example configurations
â”‚â”€â”€ data/               # Dataset preprocessing utilities
â”‚â”€â”€ results/            # Experimental results and logs
â”‚â”€â”€ requirements.txt    # Dependencies
â”‚â”€â”€ train.py            # Entry point for training
â”‚â”€â”€ eval.py             # Evaluation scripts
â””â”€â”€ README.md           # This file
```

---

## ğŸ“– Citation

If you use this code, please cite our paper:

```bibtex
@article{simoni2025gtpo,
  title={GTPO: Trajectory-Based Policy Optimization in Large Language Models},
  author={Simoni, Marco and Fontana, Aleksandar and Rossolini, Giulio and Saracino, Andrea},
  journal={arXiv preprint arXiv:2508.03772},
  year={2025}
}
```

---

## ğŸ¤ Acknowledgements

This work was carried out at:

* **Institute of Informatics and Telematics, CNR, Italy**
* **Department of Excellence in Robotics and AI, TeCIP, Scuola Superiore Santâ€™Anna**
* **National Doctorate on Artificial Intelligence, Sapienza UniversitÃ  di Roma**

---

## ğŸ“¬ Contact

For questions or collaborations, please contact:

* Marco Simoni â€“ [marco.simoni@iit.cnr.it](mailto:marco.simoni@iit.cnr.it)
* Aleksandar Fontana â€“ [aleksandar.fontana@santannapisa.it](mailto:aleksandar.fontana@santannapisa.it)
* Giulio Rossolini â€“ [giulio.rossolini@santannapisa.it](mailto:giulio.rossolini@santannapisa.it)
* Andrea Saracino â€“ [andrea.saracino@santannapisa.it](mailto:andrea.saracino@santannapisa.it)

```

